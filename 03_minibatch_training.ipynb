{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from exp.nb_02 import *\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train,x_valid,y_valid = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n,m = x_train.shape\n",
    "c = y_train.max()+1\n",
    "nh = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.layers = [nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out)]\n",
    "    def __call__(self, x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000: 784, 50, 10\n"
     ]
    }
   ],
   "source": [
    "print(f'{n}: {m}, {nh}, {c}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(m, nh, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will need to compute the softmax of our activations. This is defined by:\n",
    "\n",
    "$$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{e^{x_{0}} + e^{x_{1}} + \\cdots + e^{x_{n-1}}}$$\n",
    "\n",
    "or more concisely:\n",
    "\n",
    "$$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{\\sum_{0 \\leq j \\leq n-1} e^{x_{j}}}$$ \n",
    "\n",
    "In practice, we will need the log of the softmax when we calculate the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x): return (x.exp()/(x.exp().sum(-1,keepdim=True))).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_pred = log_softmax(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross entropy loss for some target $x$ and some prediction $p(x)$ is given by:\n",
    "\n",
    "$$ -\\sum x\\, \\log p(x) $$\n",
    "\n",
    "But since our $x$s are 1-hot encoded, this can be rewritten as $-\\log(p_{i})$ where i is the index of the desired target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be done using numpy-style [integer array indexing](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html#integer-array-indexing). Note that PyTorch supports all the tricks in the advanced indexing methods discussed in that link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.3596, grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_pred[0][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3810, -2.1905, -2.1806, -2.2228, -2.3697, -2.3596, -2.2990, -2.4769,\n",
       "         -2.2610, -2.3245],\n",
       "        [-2.2907, -2.1773, -2.2537, -2.1871, -2.4425, -2.3643, -2.3678, -2.4314,\n",
       "         -2.1852, -2.3733],\n",
       "        [-2.2076, -2.1529, -2.2123, -2.2220, -2.4739, -2.2872, -2.4041, -2.4766,\n",
       "         -2.2467, -2.4067]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_pred[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.3596, -2.2907, -2.4739], grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_pred[[0,1,2], [5,0,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(pred, target): return -pred[range(target.shape[0]), target].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.3165, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_pred[range(y_train.shape[0]), y_train].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nll(sm_pred, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3165, grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the formula \n",
    "\n",
    "$$\\log \\left ( \\frac{a}{b} \\right ) = \\log(a) - \\log(b)$$ \n",
    "\n",
    "gives a simplification when we compute the log softmax, which was previously defined as `(x.exp()/(x.exp().sum(-1,keepdim=True))).log()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x): return x - x.exp().sum(-1,keepdim=True).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_near(nll(log_softmax(pred), y_train), loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, there is a way to compute the log of the sum of exponentials in a more stable way, called the [LogSumExp trick](https://en.wikipedia.org/wiki/LogSumExp). The idea is to use the following formula:\n",
    "\n",
    "$$\\log \\left ( \\sum_{j=1}^{n} e^{x_{j}} \\right ) = \\log \\left ( e^{a} \\sum_{j=1}^{n} e^{x_{j}-a} \\right ) = a + \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}-a} \\right )$$\n",
    "\n",
    "where a is the maximum of the $x_{j}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(x):\n",
    "    m = x.max(-1)[0]\n",
    "    return m + (x-m[:,None]).exp().sum(-1).log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way, we will avoid an overflow when taking the exponential of a big activation. In PyTorch, this is already implemented for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_near(logsumexp(pred), pred.logsumexp(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can use it for our `log_softmax` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x): return x - x.logsumexp(-1,keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_near(nll(log_softmax(pred), y_train), loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then use PyTorch's implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_near(F.nll_loss(F.log_softmax(pred, -1), y_train), loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, `F.log_softmax` and `F.nll_loss` are combined in one optimized function, `F.cross_entropy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_near(F.cross_entropy(pred, y_train), loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically the training loop repeats over the following steps:\n",
    "- get the output of the model on a batch of inputs\n",
    "- compare the output to the labels we have and compute a loss\n",
    "- calculate the gradients of the loss with respect to every parameter of the model\n",
    "- update said parameters with those gradients to make them a little bit better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def accuracy(out, yb): return (torch.argmax(out, dim=1)==yb).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.1095,  0.0809,  0.0909,  0.0486, -0.0982, -0.0882, -0.0276, -0.2055,\n",
       "          0.0105, -0.0530], grad_fn=<SelectBackward>), torch.Size([64, 10]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs=64                  # batch size\n",
    "\n",
    "xb = x_train[0:bs]     # a mini-batch from x\n",
    "preds = model(xb)      # predictions\n",
    "preds[0], preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3200, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb = y_train[0:bs]\n",
    "loss_func(preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1562)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.5   # learning rate\n",
    "epochs = 1 # how many epochs to train for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range((n-1)//bs + 1):\n",
    "#         set_trace()\n",
    "        start_i = i*bs\n",
    "        end_i = start_i+bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        loss = loss_func(model(xb), yb)\n",
    "\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            for l in model.layers:\n",
    "                if hasattr(l, 'weight'):\n",
    "                    l.weight -= l.weight.grad * lr\n",
    "                    l.bias   -= l.bias.grad   * lr\n",
    "                    l.weight.grad.zero_()\n",
    "                    l.bias  .grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1718, grad_fn=<NllLossBackward>), tensor(0.9375))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using parameters and optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `nn.Module.__setattr__` and move relu to functional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(n_in,nh)\n",
    "        self.l2 = nn.Linear(nh,n_out)\n",
    "        \n",
    "    def __call__(self, x): return self.l2(F.relu(self.l1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(m, nh, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1: Linear(in_features=784, out_features=50, bias=True)\n",
      "l2: Linear(in_features=50, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "for name,l in model.named_children(): print(f\"{name}: {l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (l1): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (l2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=784, out_features=50, bias=True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((n-1)//bs + 1):\n",
    "            start_i = i*bs\n",
    "            end_i = start_i+bs\n",
    "            xb = x_train[start_i:end_i]\n",
    "            yb = y_train[start_i:end_i]\n",
    "            loss = loss_func(model(xb), yb)\n",
    "\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters(): p -= p.grad * lr\n",
    "                model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0639, grad_fn=<NllLossBackward>), tensor(1.))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit()\n",
    "loss_func(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Behind the scenes, PyTorch overrides the `__setattr__` function in `nn.Module` so that the submodules you define are properly registered as parameters of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyModule():\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        self._modules = {}\n",
    "        self.l1 = nn.Linear(n_in,nh)\n",
    "        self.l2 = nn.Linear(nh,n_out)\n",
    "        \n",
    "    def __setattr__(self,k,v):\n",
    "        if not k.startswith(\"_\"): self._modules[k] = v\n",
    "        super().__setattr__(k,v)\n",
    "        \n",
    "    def __repr__(self): return f'{self._modules}'\n",
    "    \n",
    "    def parameters(self):\n",
    "        for l in self._modules.values():\n",
    "            for p in l.parameters(): yield p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l1': Linear(in_features=784, out_features=50, bias=True), 'l2': Linear(in_features=50, out_features=10, bias=True)}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl = DummyModule(m,nh,10)\n",
    "mdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([50, 784]),\n",
       " torch.Size([50]),\n",
       " torch.Size([10, 50]),\n",
       " torch.Size([10])]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[o.shape for o in mdl.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the original `layers` approach, but we have to register the modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        for i,l in enumerate(self.layers): self.add_module(f'layer_{i}', l)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (layer_0): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (layer_1): ReLU()\n",
       "  (layer_2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.ModuleList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.ModuleList` does this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialModel(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SequentialModel(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialModel(\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=784, out_features=50, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=50, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1376, grad_fn=<NllLossBackward>), tensor(0.9375))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit()\n",
    "loss_func(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.Sequential` is a convenient class which does the same as the above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0832, grad_fn=<NllLossBackward>), tensor(1.))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit()\n",
    "loss_func(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m        \n",
       "\u001b[0;32mclass\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34mr\"\"\"A sequential container.\u001b[0m\n",
       "\u001b[0;34m    Modules will be added to it in the order they are passed in the constructor.\u001b[0m\n",
       "\u001b[0;34m    Alternatively, an ordered dict of modules can also be passed in.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    To make it easier to understand, here is a small example::\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        # Example of using Sequential\u001b[0m\n",
       "\u001b[0;34m        model = nn.Sequential(\u001b[0m\n",
       "\u001b[0;34m                  nn.Conv2d(1,20,5),\u001b[0m\n",
       "\u001b[0;34m                  nn.ReLU(),\u001b[0m\n",
       "\u001b[0;34m                  nn.Conv2d(20,64,5),\u001b[0m\n",
       "\u001b[0;34m                  nn.ReLU()\u001b[0m\n",
       "\u001b[0;34m                )\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        # Example of using Sequential with OrderedDict\u001b[0m\n",
       "\u001b[0;34m        model = nn.Sequential(OrderedDict([\u001b[0m\n",
       "\u001b[0;34m                  ('conv1', nn.Conv2d(1,20,5)),\u001b[0m\n",
       "\u001b[0;34m                  ('relu1', nn.ReLU()),\u001b[0m\n",
       "\u001b[0;34m                  ('conv2', nn.Conv2d(20,64,5)),\u001b[0m\n",
       "\u001b[0;34m                  ('relu2', nn.ReLU())\u001b[0m\n",
       "\u001b[0;34m                ]))\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_get_item_by_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Get the idx-th item of the iterator\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'index {} is out of range'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0midx\u001b[0m \u001b[0;34m%=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_by_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_by_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__delitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mdelattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_by_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mdelattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m           ~/anaconda3/envs/fastai/lib/python3.7/site-packages/torch/nn/modules/container.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn.Sequential??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's replace our previous manually coded optimization step:\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    for p in model.parameters(): p -= p.grad * lr\n",
    "    model.zero_grad()\n",
    "```\n",
    "\n",
    "and instead use just:\n",
    "\n",
    "```python\n",
    "opt.step()\n",
    "opt.zero_grad()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    def __init__(self, params, lr=0.5):\n",
    "        self.params,self.lr = list(params),lr\n",
    "        \n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for p in self.params: p -= p.grad * lr\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.params: p.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Optimizer(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range((n-1)//bs + 1):\n",
    "        start_i = i*bs\n",
    "        end_i = start_i+bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range((n-1)//bs+1):\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1352, grad_fn=<NllLossBackward>), tensor(0.9375))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
    "loss,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch already provides this exact functionality in `optim.SGD` (it also handles stuff like momentum, which we'll look at later - except we'll be doing it in a more flexible way!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "optim.SGD.step??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))\n",
    "    return model, optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))\n",
    "    return model, optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2763, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model,opt = get_model()\n",
    "loss_func(model(xb), yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range((n-1)//bs + 1):\n",
    "        start_i = i*bs\n",
    "        end_i = start_i+bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2085, grad_fn=<NllLossBackward>), tensor(0.9375))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
    "loss,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2085, grad_fn=<NllLossBackward>), tensor(0.9375))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
    "loss,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomized tests can be very useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert acc>0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clunky to iterate through minibatches of x and y values separately:\n",
    "\n",
    "```python\n",
    "    xb = x_train[start_i:end_i]\n",
    "    yb = y_train[start_i:end_i]\n",
    "```\n",
    "\n",
    "Instead, let's do these two steps together, by introducing a `Dataset` class:\n",
    "\n",
    "```python\n",
    "    xb,yb = train_ds[i*bs : i*bs+bs]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Dataset():\n",
    "    def __init__(self, x, y): self.x,self.y = x,y\n",
    "    def __len__(self): return len(self.x)\n",
    "    def __getitem__(self, i): return self.x[i],self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)\n",
    "assert len(train_ds)==len(x_train)\n",
    "assert len(valid_ds)==len(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([5, 0, 4, 1, 9]))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb,yb = train_ds[0:5]\n",
    "assert xb.shape==(5,28*28)\n",
    "assert yb.shape==(5,)\n",
    "xb,yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,opt = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range((n-1)//bs + 1):\n",
    "        xb,yb = train_ds[i*bs : i*bs+bs]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2071, grad_fn=<NllLossBackward>), tensor(0.9375))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
    "assert acc>0.7\n",
    "loss,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, our loop iterated over batches (xb, yb) like this:\n",
    "\n",
    "```python\n",
    "for i in range((n-1)//bs + 1):\n",
    "    xb,yb = train_ds[i*bs : i*bs+bs]\n",
    "    ...\n",
    "```\n",
    "\n",
    "Let's make our loop much cleaner, using a data loader:\n",
    "\n",
    "```python\n",
    "for xb,yb in train_dl:\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, ds, bs): self.ds,self.bs = ds,bs\n",
    "    def __iter__(self):\n",
    "        for i in range(0, len(self.ds), self.bs): yield self.ds[i:i+self.bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, bs)\n",
    "valid_dl = DataLoader(valid_ds, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = next(iter(valid_dl))\n",
    "assert xb.shape==(bs,28*28)\n",
    "assert yb.shape==(bs,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xb[0].view(28,28))\n",
    "yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,opt = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for xb,yb in train_dl:\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0788, grad_fn=<NllLossBackward>), tensor(0.9688))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
    "assert acc>0.7\n",
    "loss,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want our training set to be in a random order, and that order should differ each iteration. But the validation set shouldn't be randomized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler():\n",
    "    def __init__(self, ds, bs, shuffle=False):\n",
    "        self.n,self.bs,self.shuffle = len(ds),bs,shuffle\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.idxs = torch.randperm(self.n) if self.shuffle else torch.arange(self.n)\n",
    "        for i in range(0, self.n, self.bs): yield self.idxs[i:i+self.bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_ds = Dataset(*train_ds[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7, 8]), tensor([9])]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = Sampler(small_ds,3,False)\n",
    "[o for o in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([7, 6, 4]), tensor([1, 5, 9]), tensor([3, 8, 0]), tensor([2])]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = Sampler(small_ds,3,True)\n",
    "[o for o in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(b):\n",
    "    xs,ys = zip(*b)\n",
    "    return torch.stack(xs),torch.stack(ys)\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self, ds, sampler, collate_fn=collate):\n",
    "        self.ds,self.sampler,self.collate_fn = ds,sampler,collate_fn\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for s in self.sampler: yield self.collate_fn([self.ds[i] for i in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samp = Sampler(train_ds, bs, shuffle=True)\n",
    "valid_samp = Sampler(valid_ds, bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, sampler=train_samp, collate_fn=collate)\n",
    "valid_dl = DataLoader(valid_ds, sampler=valid_samp, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xb,yb = next(iter(valid_dl))\n",
    "plt.imshow(xb[0].view(28,28))\n",
    "yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xb,yb = next(iter(train_dl))\n",
    "plt.imshow(xb[0].view(28,28))\n",
    "yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xb,yb = next(iter(train_dl))\n",
    "plt.imshow(xb[0].view(28,28))\n",
    "yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1919, grad_fn=<NllLossBackward>), tensor(0.9688))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model,opt = get_model()\n",
    "fit()\n",
    "\n",
    "loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
    "assert acc>0.7\n",
    "loss,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, bs, sampler=RandomSampler(train_ds), collate_fn=collate)\n",
    "valid_dl = DataLoader(valid_ds, bs, sampler=SequentialSampler(valid_ds), collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1701, grad_fn=<NllLossBackward>), tensor(0.9531))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model,opt = get_model()\n",
    "fit()\n",
    "loss_func(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch's defaults work fine for most things however:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, bs, shuffle=True, drop_last=True)\n",
    "valid_dl = DataLoader(valid_ds, bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.3151, grad_fn=<NllLossBackward>), tensor(0.9219))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model,opt = get_model()\n",
    "fit()\n",
    "\n",
    "loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
    "assert acc>0.7\n",
    "loss,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that PyTorch's `DataLoader`, if you pass `num_workers`, will use multiple threads to call your `Dataset`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You **always** should also have a [validation set](http://www.fast.ai/2017/11/13/validation-sets/), in order to identify if you are overfitting.\n",
    "\n",
    "We will calculate and print the validation loss at the end of each epoch.\n",
    "\n",
    "(Note that we always call `model.train()` before training, and `model.eval()` before inference, because these are used by layers such as `nn.BatchNorm2d` and `nn.Dropout` to ensure appropriate behaviour for these different phases.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        # Handle batchnorm / dropout\n",
    "        model.train()\n",
    "#         print(model.training)\n",
    "        for xb,yb in train_dl:\n",
    "            loss = loss_func(model(xb), yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "        model.eval()\n",
    "#         print(model.training)\n",
    "        with torch.no_grad():\n",
    "            tot_loss,tot_acc = 0.,0.\n",
    "            for xb,yb in valid_dl:\n",
    "                pred = model(xb)\n",
    "                tot_loss += loss_func(pred, yb)\n",
    "                tot_acc  += accuracy (pred,yb)\n",
    "        nv = len(valid_dl)\n",
    "        print(epoch, tot_loss/nv, tot_acc/nv)\n",
    "    return tot_loss/nv, tot_acc/nv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Question*: Are these validation results correct if batch size varies?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_dls` returns dataloaders for the training and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_dls(train_ds, valid_ds, bs, **kwargs):\n",
    "    return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),\n",
    "            DataLoader(valid_ds, batch_size=bs*2, **kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our whole process of obtaining the data loaders and fitting the model can be run in 3 lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.1964) tensor(0.9388)\n",
      "1 tensor(1.5323) tensor(0.7574)\n",
      "2 tensor(0.2117) tensor(0.9392)\n",
      "3 tensor(0.3407) tensor(0.9031)\n",
      "4 tensor(0.0984) tensor(0.9700)\n"
     ]
    }
   ],
   "source": [
    "train_dl,valid_dl = get_dls(train_ds, valid_ds, bs)\n",
    "model,opt = get_model()\n",
    "loss,acc = fit(5, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert acc>0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 03_minibatch_training.ipynb to exp/nb_03.py\n"
     ]
    }
   ],
   "source": [
    "!python notebook2script.py 03_minibatch_training.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
